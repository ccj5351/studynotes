{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+Xh8fIP0157gS8OiyV5+Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccj5351/studynotes/blob/master/loading_pytorch_to_tensorflow_float32_vs_float64_precision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Pytorch model to Tensorflow by Hand\n",
        "\n",
        "> see the references that helped me when solving this task.\n",
        ">  - [Manualy convert pytorch weights to tf.keras weights for convolutional layer](https://stackoverflow.com/questions/68165375/manualy-convert-pytorch-weights-to-tf-keras-weights-for-convolutional-layer)\n",
        ">  - [Pytorch convolution and tensorflow convolution giving different results](https://discuss.pytorch.org/t/pytorch-convolution-and-tensorflow-convolution-giving-different-results/26863)\n",
        ">  - [A single conv and same input leads to results different from tensorflow](https://discuss.pytorch.org/t/a-single-conv-and-same-input-leads-to-results-different-from-tensorflow/17735/2)\n",
        ">  - [How to set weights in Keras with a numpy array?](https://stackoverflow.com/questions/47183159/how-to-set-weights-in-keras-with-a-numpy-array)\n",
        ">  - [Copying weight tensors from PyTorch to Tensorflow (and back)](https://www.adrian.idv.hk/2022-05-21-torch2tf/)\n",
        ">  - [Converting a Simple Deep Learning Model from PyTorch to TensorFlow](https://towardsdatascience.com/converting-a-simple-deep-learning-model-from-pytorch-to-tensorflow-b6b353351f5d)\n",
        "\n",
        "In this tutorial we will load the pretrained PyTorch (PT) model weights to the TensorFlow (TF) model by hand.\n",
        "\n",
        "### Take-home message:\n",
        "- [x]: batch input dimension in TensorFlow: NHWC, PyTorch: NCHW, and convolution weights in TF: [H, W, C$_i$, C$_o$], PyTorch [C$_o$, C$_i$, H, W]; So when assign PyTorch weights to TF, you have to transpose the weights accordingly.\n",
        "- [x]: `float32` vs `float64`. Even if you have already assigned the weights correctly from PT to TF, probably the their resutls are different due to floa"
      ],
      "metadata": {
        "id": "Y-B86DUbEY-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "LKzHgRDW_l6B"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Module - TensorFlow Code"
      ],
      "metadata": {
        "id": "m6P3m5KBIbIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pdb\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "class FeatureExtractor(tf.keras.layers.Layer):\n",
        "    \"\"\"Feature extraction\n",
        "    Args:\n",
        "        C: list of number of feature channels, C=[C0,C1,C2,C3,C4],\n",
        "        e.g., C=[16,16,24,24,32];\n",
        "    Returns: a list of features at different resolutions, including:\n",
        "        o4: tensor in shape of [B,C0,H,W]\n",
        "        o3: tensor in shape of [B,C1,H/2,W/2]\n",
        "        o2: tensor in shape of [B,C2,H/4,W/4]\n",
        "        o1: tensor in shape of [B,C3,H/8,W/8]\n",
        "        o0: tensor in shape of [B,C4,H/16,W/16]\n",
        "    \"\"\"\n",
        "    def __init__(self, C):\n",
        "        super().__init__()\n",
        "        assert len(C) in [5], f\"Have {len(C)} scales of features\"\n",
        "        #self.down_0 = models.Sequential([\n",
        "        #    layers.Conv2D(\n",
        "        #        C[0], #output_planes\n",
        "        #        kernel_size=3,\n",
        "        #        strides=1,\n",
        "        #        padding='same',\n",
        "        #        use_bias=True\n",
        "        #        ),\n",
        "        #    layers.LeakyReLU(alpha=0.2)\n",
        "        #    ])\n",
        "\n",
        "        self.down_0 = layers.Conv2D(\n",
        "                C[0], #output_planes\n",
        "                kernel_size=3,\n",
        "                strides=1,\n",
        "                padding='same',\n",
        "                use_bias=True\n",
        "                )\n",
        "\n",
        "        self.down_1 = models.Sequential([\n",
        "            layers.Conv2D(C[1], kernel_size=4, strides=2, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(C[1], kernel_size=3, strides=1, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2)\n",
        "        ])\n",
        "\n",
        "        self.down_2 = models.Sequential([\n",
        "            layers.Conv2D(C[2], kernel_size=4, strides=2, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(C[2], kernel_size=3, strides=1, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2)\n",
        "        ])\n",
        "\n",
        "        self.down_3 = models.Sequential([\n",
        "            layers.Conv2D(C[3], kernel_size=4, strides=2, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(C[3], kernel_size=3, strides=1, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2)\n",
        "        ])\n",
        "\n",
        "        self.down_4 = models.Sequential([\n",
        "            layers.Conv2D(C[4], kernel_size=4, strides=2, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(C[4], kernel_size=3, strides=1, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(C[4], kernel_size=3, strides=1, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(C[4], kernel_size=3, strides=1, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2)\n",
        "        ])\n",
        "\n",
        "        self.up_3 = UpsampleBlock(C[4], C[3])\n",
        "        self.up_2 = UpsampleBlock(C[3], C[2])\n",
        "        self.up_1 = UpsampleBlock(C[2], C[1])\n",
        "        self.up_0 = UpsampleBlock(C[1], C[0])\n",
        "\n",
        "    def call(self, input):\n",
        "        x0 = self.down_0(input) # [B,H,W,C0]\n",
        "        print (f\"tf x0 shape = {x0.shape}, some values = {x0[0,:5,:5,0]}\")\n",
        "        x1 = self.down_1(x0) # [B,H/2,W/2,C1]\n",
        "        x2 = self.down_2(x1) # [B,H/4,W/4,C2]\n",
        "        x3 = self.down_3(x2) # [B,H/8,W/8,C3]\n",
        "        o0 = self.down_4(x3) # [B,H/16,W/16,C4]\n",
        "\n",
        "        o1 = self.up_3(o0, x3) #[B,H/8,W/8,C3]\n",
        "        o2 = self.up_2(o1, x2) #[B,H/4,W/4,C2]\n",
        "        o3 = self.up_1(o2, x1) #[B,H/2,W/2,C1]\n",
        "        o4 = self.up_0(o3, x0) #[B,H,W,C0]\n",
        "        #pdb.set_trace()\n",
        "        #return o4, o3, o2, o1, o0\n",
        "        res = {\n",
        "            'x0': x0,\n",
        "            'x1': x1,\n",
        "            'x2': x2,\n",
        "            'x3': x3,\n",
        "            'o0': o0,\n",
        "            'o1': o1,\n",
        "            'o2': o2,\n",
        "            'o3': o3,\n",
        "            'o4': o4,\n",
        "        }\n",
        "        return res\n",
        "\n",
        "class UpsampleBlock(layers.Layer):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.c0 = in_channels\n",
        "        c1 = out_channels\n",
        "        #self.up_conv = models.Sequential([\n",
        "        self.up_conv = tf.keras.Sequential([\n",
        "            layers.Conv2DTranspose(c1, kernel_size=2, strides=2, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2)\n",
        "        ])\n",
        "\n",
        "        #self.merge_conv = models.Sequential([\n",
        "        self.merge_conv = tf.keras.Sequential([\n",
        "            layers.Conv2D(c1, kernel_size=1, padding='valid', data_format='channels_last'),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "            layers.Conv2D(c1, kernel_size=3, strides=1, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "            layers.Conv2D(c1, kernel_size=3, strides=1, padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2)\n",
        "        ])\n",
        "\n",
        "    def call(self, input, skip):\n",
        "        print (input.shape, self.c0)\n",
        "        #assert input.shape[-1] == self.c0, \"Wrong channels dimensionality\"\n",
        "        x = self.up_conv(input)\n",
        "        x = tf.concat([x, skip], axis=-1) # channels_last\n",
        "        x = self.merge_conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "n7Z68poMgZJZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Module - PyTorch Code"
      ],
      "metadata": {
        "id": "rR_n4s2_O7R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeatureExtractor_PT(nn.Module):\n",
        "    def __init__(self, C):\n",
        "        \"\"\"Feature extraction\n",
        "        Args:\n",
        "            C: list of number of feature channels, C=[C0,C1,C2,C3,C4],\n",
        "               e.g., C=[16,16,24,24,32];\n",
        "        Returns: a list of features at different resolutions, including:\n",
        "            o4: tensor in shape of [B,C0,H,W]\n",
        "            o3: tensor in shape of [B,C1,H/2,W/2]\n",
        "            o2: tensor in shape of [B,C2,H/4,W/4]\n",
        "            o1: tensor in shape of [B,C3,H/8,W/8]\n",
        "            o0: tensor in shape of [B,C4,H/16,W/16]\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert len(C) in [5], f\"Have {len(C)} scales of features\"\n",
        "        #self.down_0 = nn.Sequential(\n",
        "        #    nn.Conv2d(3, C[0], kernel_size=3, stride=1, padding=1,\n",
        "        #            bias=True # by default;\n",
        "        #            ),\n",
        "        #    nn.LeakyReLU(0.2),\n",
        "        #)\n",
        "        self.down_0 = nn.Conv2d(3, C[0], kernel_size=3, stride=1, padding=1,\n",
        "                    bias=True # by default;\n",
        "                    )\n",
        "\n",
        "        self.down_1 = nn.Sequential(\n",
        "            #SameConv2d(C[0], C[1], kernel_size=4, stride=2),\n",
        "            nn.Conv2d(C[0], C[1], kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(C[1], C[1], kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.down_2 = nn.Sequential(\n",
        "            #SameConv2d(C[1], C[2], 4, 2),\n",
        "            nn.Conv2d(C[1], C[2], kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(C[2], C[2], kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.down_3 = nn.Sequential(\n",
        "            #SameConv2d(C[2], C[3], 4, 2),\n",
        "            nn.Conv2d(C[2], C[3], kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(C[3], C[3], kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.down_4 = nn.Sequential(\n",
        "            #SameConv2d(C[3], C[4], 4, 2),\n",
        "            nn.Conv2d(C[3], C[4], kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(C[4], C[4], kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(C[4], C[4], kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(C[4], C[4], kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.up_3 = UpsampleBlock_PT(C[4], C[3])\n",
        "        self.up_2 = UpsampleBlock_PT(C[3], C[2])\n",
        "        self.up_1 = UpsampleBlock_PT(C[2], C[1])\n",
        "        self.up_0 = UpsampleBlock_PT(C[1], C[0])\n",
        "\n",
        "    def forward(self, input):\n",
        "        x0 = self.down_0(input) # [B,C0,H,W]\n",
        "        print (f\"Pytorch x0 shape = {x0.shape}, some values = {x0[0,0, :5,:5]}\")\n",
        "        x1 = self.down_1(x0) # [B,C1,H/2,W/2]\n",
        "        x2 = self.down_2(x1) # [B,C2,H/4,W/4]\n",
        "        x3 = self.down_3(x2) # [B,C3,H/8,W/8]\n",
        "        o0 = self.down_4(x3) # [B,C4,H/16,W/16]\n",
        "\n",
        "        o1 = self.up_3(o0, x3) #[B,C3,H/8,W/8]\n",
        "        o2 = self.up_2(o1, x2) #[B,C2,H/4,W/4]\n",
        "        o3 = self.up_1(o2, x1) #[B,C1,H/2,W/2]\n",
        "        o4 = self.up_0(o3, x0) #[B,C0,H,W]\n",
        "        #pdb.set_trace()\n",
        "        #return o4, o3, o2, o1, o0\n",
        "        res = {\n",
        "            'x0': x0,\n",
        "            'x1': x1,\n",
        "            'x2': x2,\n",
        "            'x3': x3,\n",
        "            'o0': o0,\n",
        "            'o1': o1,\n",
        "            'o2': o2,\n",
        "            'o3': o3,\n",
        "            'o4': o4,\n",
        "        }\n",
        "        return res\n",
        "\n",
        "class UpsampleBlock_PT(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        c0 = in_channels\n",
        "        c1 = out_channels\n",
        "        self.up_conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(c0, c1, kernel_size=2, stride=2),\n",
        "            nn.LeakyReLU(negative_slope=0.2),\n",
        "        )\n",
        "        self.merge_conv = nn.Sequential(\n",
        "            nn.Conv2d(c1 * 2, c1, kernel_size=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, skip):\n",
        "        x = self.up_conv(input)\n",
        "        ## should be in the same resolution now\n",
        "        #if x.size()[2:] != skip.size()[2:]:\n",
        "        #    x = x[:, :, : skip.size(2), : skip.size(3)]\n",
        "        x = torch.cat((x, skip), dim=1)\n",
        "        x = self.merge_conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qaUz9g7NtvVs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input and Set Floating Point Precision\n",
        "\n",
        "- Set the float type in TensorFlow via `tf.keras.backend.set_floatx('float64')` or `tf.keras.backend.set_floatx('float32')`.\n",
        "\n",
        "- Set the float type in PyTorch via `torch.set_default_dtype(torch.float64)` or `torch.set_default_dtype(torch.float32)`."
      ],
      "metadata": {
        "id": "rHroZzFLPElz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(88883)\n",
        "num_feature = [16, 16, 24, 24, 32]\n",
        "\n",
        "USING_FLOAT64 = True\n",
        "\n",
        "if USING_FLOAT64:\n",
        "  tf.keras.backend.set_floatx('float64')\n",
        "  torch.set_default_dtype(torch.float64)\n",
        "  DATA_TYPE = np.float64\n",
        "else:\n",
        "  tf.keras.backend.set_floatx('float32')\n",
        "  torch.set_default_dtype(torch.float32)\n",
        "  DATA_TYPE = np.float32\n",
        "\n",
        "height = 128\n",
        "width = 256\n",
        "\n",
        "#prepare inputs and do inference\n",
        "x = np.random.rand(1, 3, height, width).astype(DATA_TYPE)\n",
        "x_tf = np.transpose(x, (0, 2, 3, 1)).astype(DATA_TYPE)"
      ],
      "metadata": {
        "id": "jXjpRexEixCJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an instance of the feature extractor in TensorFlow first."
      ],
      "metadata": {
        "id": "1r9XkVYxQN94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = FeatureExtractor(num_feature)\n",
        "lf = feature_extractor(x_tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aRXhYeqQPoi",
        "outputId": "20987fed-dd73-4244-d968-df6810239795"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf x0 shape = (1, 128, 256, 16), some values = [[-0.11584787 -0.15231569 -0.2665561  -0.15198092 -0.22057036]\n",
            " [-0.28681828 -0.5179587  -0.41952042 -0.3106658  -0.36161061]\n",
            " [-0.06758231 -0.3103331  -0.39738146 -0.35538858 -0.4615705 ]\n",
            " [-0.1607221  -0.30408457 -0.64614803 -0.09946881 -0.55594852]\n",
            " [-0.08477888 -0.36469683 -0.36448385 -0.3245575  -0.33466563]]\n",
            "(1, 8, 16, 32) 32\n",
            "(1, 16, 32, 24) 24\n",
            "(1, 32, 64, 24) 24\n",
            "(1, 64, 128, 16) 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then create another instance of the feature extractor in Pytorch."
      ],
      "metadata": {
        "id": "RAdgRCbNQoa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(precision=8)\n",
        "feature_extractor_pt = FeatureExtractor_PT(num_feature)"
      ],
      "metadata": {
        "id": "9fFi120b9O-7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to specify the weights of first convolution layer by hand, do the following assignment."
      ],
      "metadata": {
        "id": "3_QUlQwCQ3eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RESET_LAYER_ZERO = True\n",
        "if RESET_LAYER_ZERO:\n",
        "  out_plane = num_feature[0]\n",
        "  in_plane = 3\n",
        "  k = 3\n",
        "  torch_weights = np.random.rand(out_plane, in_plane, k, k).astype(DATA_TYPE)\n",
        "  bias = np.random.rand(out_plane).astype(DATA_TYPE)\n",
        "  tf_weights = np.transpose(torch_weights, (2, 3, 1, 0)).astype(DATA_TYPE)\n",
        "\n",
        "  conv0 = getattr(feature_extractor_pt, 'down_0')\n",
        "  conv0.weight = torch.nn.Parameter(torch.from_numpy(torch_weights))\n",
        "  conv0.bias = torch.nn.Parameter(torch.from_numpy(bias))"
      ],
      "metadata": {
        "id": "Qo0qT8tZQ4vy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_pt = torch.from_numpy(x)\n",
        "print (\"PyTorch ...\")\n",
        "lf_pt = feature_extractor_pt(x_pt)\n",
        "print (conv0.weight.dtype, conv0.bias.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maW7eJjfGmYz",
        "outputId": "bc61d123-b65a-4f0a-fbef-70e4df392509"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch ...\n",
            "Pytorch x0 shape = torch.Size([1, 16, 128, 256]), some values = tensor([[3.91926824, 5.81627401, 5.01474286, 4.82458783, 5.29005626],\n",
            "        [6.21916518, 7.65551395, 7.66813092, 6.18409405, 6.82858778],\n",
            "        [5.76677646, 7.06619377, 7.27656830, 6.16707753, 7.14074652],\n",
            "        [4.88346984, 6.88705355, 6.03917426, 8.72257558, 7.32111244],\n",
            "        [5.58896538, 7.20987216, 7.36217130, 7.87393225, 8.03545963]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.float64 torch.float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load pytorch weights to Tensorflow. We can save Pytorch checkpoint to a json file for TensorFlow model to load later."
      ],
      "metadata": {
        "id": "AK_gV3SgRL_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load pytorch weights to Tensorflow\n",
        "# save Pytorch checkpoint to json for TensorFlow model to load;\n",
        "import json\n",
        "\n",
        "#Start by connecting gdrive into the google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Now your Google Drive is mounted to this location /content/gdrive/My Drive/\n",
        "#!ls \"/content/drive/My Drive/\"\n",
        "\n",
        "ckpt = feature_extractor_pt.state_dict()\n",
        "n = 0\n",
        "to_save = {}\n",
        "\n",
        "for i, (k, v) in enumerate(ckpt.items()):\n",
        "  print (f\"idx={n}, {k} ==> ({v.dtype}, {v.shape})\")\n",
        "  n += 1\n",
        "  # Convert numpy arrays to list\n",
        "  tmp = v.numpy().tolist()\n",
        "  to_save[k] = tmp\n",
        "  if i <= 1:\n",
        "    print (f\"ckpt idx {i} , weight = {tmp}\")\n",
        "\n",
        "json_file = f'/content/drive/My Drive/Colab Notebooks/hitnet_sf_finalpass_{DATA_TYPE}.json'\n",
        "with open(json_file, 'w') as f:\n",
        "  json.dump(to_save, f, indent=2)\n",
        "\n",
        "\n",
        "# To convert back:\n",
        "# Load from JSON\n",
        "# Open the file and load the JSON\n",
        "with open(json_file, 'r') as f:\n",
        "  dict_from_json = json.load(f)\n",
        "\n",
        "# Convert lists back to numpy arrays\n",
        "final_dict = {k: np.array(v).astype(DATA_TYPE) for k, v in dict_from_json.items()}\n",
        "\n",
        "# Check if dictionaries are equal\n",
        "is_equal = all(np.array_equal(ckpt[key], final_dict[key]) for key in ckpt)\n",
        "print(is_equal)  # Should print True if dictionaries are equal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45skbVow_9Q1",
        "outputId": "2b9251d8-039a-47ed-d4c6-8441e183fc1a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "idx=0, down_0.weight ==> (torch.float64, torch.Size([16, 3, 3, 3]))\n",
            "ckpt idx 0 , weight = [[[[0.2664833371725843, 0.28596531755435894, 0.9472216837134946], [0.795540222664237, 0.5040244018089428, 0.8308837052259994], [0.39511066786724236, 0.8915322817894845, 0.028891086665465915]], [[0.4779673328439302, 0.10626933349568268, 0.3119829087011352], [0.38376573311536444, 0.4169593055194456, 0.07177749308303671], [0.24003620457041253, 0.2458529117434346, 0.6279410046075949]], [[0.942160709602115, 0.4055751917571998, 0.8576588887014762], [0.049824236984481685, 0.9923666749333885, 0.7234285356179979], [0.10893476096898191, 0.3135176494076015, 0.146820772255298]]], [[[0.22243685297545068, 0.4005424960573717, 0.49303269210182443], [0.9254500301558617, 0.1717217171153318, 0.5478600232885603], [0.36977001638776696, 0.9259932503883489, 0.056735806325113214]], [[0.39003620448974186, 0.9248602462924321, 0.63758089742486], [0.4273297725556736, 0.3615828284146677, 0.5529689709576913], [0.12628266331483373, 0.04510663530613079, 0.9382430613863736]], [[0.01475775829314041, 0.9871215681510099, 0.07473501221585765], [0.27301471901534335, 0.3470305721485458, 0.1836794466403051], [0.20533874515155537, 0.5061418157102793, 0.03514428728717778]]], [[[0.3584399197363396, 0.8126396296977162, 0.8021892369018058], [0.43978597095397376, 0.31963825383824085, 0.22918723790391415], [0.06683792131165667, 0.7380739645328969, 0.3839813923286701]], [[0.2261281009346343, 0.521905097716629, 0.8622038513108976], [0.8423469509395924, 0.4411476273955367, 0.9043483393915712], [0.10135040020894426, 0.5512789137092996, 0.2859508667529709]], [[0.28595530010368264, 0.11528219028948372, 0.7916590829161809], [0.963130484503226, 0.5934964622889798, 0.6871980684557786], [0.21678634216031822, 0.8950978869743913, 0.8052355207190258]]], [[[0.566815205356278, 0.2941390134310118, 0.7551094928948381], [0.9425035626082945, 0.5664876590787236, 0.5838660081746555], [0.07999390942266638, 0.3801802874834207, 0.9701915313432092]], [[0.7037254726592691, 0.602870504050115, 0.9527735424232812], [0.8311296907727831, 0.4214877593872245, 0.787722939285074], [0.37843513510409876, 0.14849116126590156, 0.19650867560704555]], [[0.9934526421201634, 0.1558719334698725, 0.21486046250396185], [0.457491467706846, 0.8282876922864626, 0.7188286040025048], [0.01078948705804672, 0.2550790939579788, 0.33271982727890737]]], [[[0.36250575240008986, 0.12423220154192427, 0.06086632157492544], [0.2041056825127091, 0.52759405347629, 0.13314931909034244], [0.6940325116057504, 0.4056328434422929, 0.5157509322170496]], [[0.576192348964793, 0.1366456572771949, 0.2012509666716772], [0.9865977694646331, 0.3954501930889912, 0.7292360641992557], [0.7757990270994617, 0.8334725505565275, 0.29228277721259777]], [[0.8227973587321005, 0.2548055802002043, 0.5293110374933641], [0.3172852346120075, 0.2147803493336149, 0.8204124867514773], [0.09583400348389048, 0.9127567465660348, 0.9049511779720714]]], [[[0.3711260211603572, 0.08014840698737014, 0.20597678301085132], [0.7995780001975052, 0.776343250222745, 0.7409987654971262], [0.7063853458238556, 0.4391970525575293, 0.8446814720519992]], [[0.6122631696420424, 0.3286406778957529, 0.5271119514356826], [0.8237514735434649, 0.0589607789587413, 0.821514500561196], [0.9010469848920942, 0.9873742610825289, 0.32731384072373804]], [[0.9683260023522694, 0.9926434229979106, 0.3108491376014897], [0.8349452513432448, 0.11836310791020477, 0.12191658033101216], [0.6809858979410996, 0.3607311988631925, 0.014000122431811368]]], [[[0.7625775047369527, 0.6105933327434918, 0.3665323847562215], [0.28647028943275477, 0.2861162166593709, 0.829291036283313], [0.5246458885723314, 0.2902334084085175, 0.04116033834812838]], [[0.6847850928167123, 0.6921669630093435, 0.40751272661085647], [0.4119143922251459, 0.1292307710948286, 0.03347172993435765], [0.0005960502340420781, 0.3402641953046075, 0.8451031876254256]], [[0.9126230158965636, 0.2281625511950346, 0.5709107886900388], [0.16841384228748957, 0.9944028670488774, 0.6552594807430386], [0.11114606325811349, 0.4019169633561801, 0.16771864629819055]]], [[[0.3763053122126494, 0.8953868146164075, 0.7059041078768723], [0.00753450399106026, 0.881900618236713, 0.017894071169215153], [0.1954369183351763, 0.5443457017193469, 0.0431257266770132]], [[0.36855258804820357, 0.11011825113554441, 0.6688756431162822], [0.3834993507590342, 0.26506069062942084, 0.2976986134715899], [0.08475554684950437, 0.6686148745398297, 0.0019152076912574145]], [[0.1937750343161534, 0.0911368019665122, 0.06395678105317737], [0.0054753280304680185, 0.6921928667554426, 0.3229099770664291], [0.4511100498266648, 0.8358022481369092, 0.6036462922640993]]], [[[0.4638729232142105, 0.2211554234614942, 0.7990263755790306], [0.38901394565891, 0.9654238045454403, 0.03076971429534625], [0.15746317681996902, 0.6590299039039672, 0.06986874916565877]], [[0.17646117356295798, 0.3997490640192437, 0.9380803641544312], [0.4559140441236261, 0.09419638785509776, 0.48560033725513185], [0.6245653258895146, 0.25954539412006217, 0.4266657954331816]], [[0.8985807568718901, 0.2826778418020802, 0.8674999204352934], [0.12785202353887937, 0.027579978817398265, 0.9148840294671845], [0.738374278085032, 0.1859934501997882, 0.6624976371907038]]], [[[0.4378942524225249, 0.713288740188442, 0.31381648486680846], [0.8834857052635836, 0.2987540347391736, 0.8984572151053468], [0.042055623040817225, 0.8811143424551391, 0.4898633898581677]], [[0.889075668816782, 0.9119293951976093, 0.5412521638108094], [0.0441296579834467, 0.41883928059954256, 0.40041129832236355], [0.8367752688310685, 0.2500567205752704, 0.3231186643666799]], [[0.1753552119243057, 0.2440171279791279, 0.738242348483932], [0.962808211468963, 0.25133115595602795, 0.04275669827214512], [0.23372454317014424, 0.21934117274152864, 0.6105154410870112]]], [[[0.69200099775016, 0.908468355545868, 0.7655922406017046], [0.303902738085422, 0.1589940537481942, 0.28176402432605097], [0.32157462514840796, 0.7023498616540613, 0.7874445414983022]], [[0.7601983135569006, 0.7484255530069874, 0.2737607498508762], [0.3168381688538846, 0.8080787503665927, 0.6022902458208917], [0.5477864086014107, 0.48114610367593036, 0.25252871085707806]], [[0.61250988393666, 0.7656336785038077, 0.8396980358683035], [0.5800546485853907, 0.9305535227333329, 0.8187869482691252], [0.7711368187883989, 0.25405030233865733, 0.03757024957926147]]], [[[0.8030748010522714, 0.5434745816744518, 0.3375000099407267], [0.011217301937923252, 0.6990893537630899, 0.10284386323414985], [0.5152110873065582, 0.4622846769441965, 0.5025257171958752]], [[0.8331649305307496, 0.981826219380519, 0.32016679617310106], [0.7111710397704027, 0.3624025440401325, 0.24851299707327634], [0.27274411140396493, 0.5552189832909626, 0.6651534581972792]], [[0.9881358968167427, 0.8866734190896074, 0.8973579719045577], [0.18981347516342828, 0.9267527103352696, 0.0020766476116326205], [0.010953753570325908, 0.07794055688984391, 0.6615441455480703]]], [[[0.6045312307389061, 0.8062280922257773, 0.23450923882914854], [0.3716243377608677, 0.4261900046606626, 0.3253892685953893], [0.8703498236943848, 0.8012879621559195, 0.2806187104529415]], [[0.8965563846476833, 0.4118884224718399, 0.1866162129882485], [0.3103933721770522, 0.38965129232543627, 0.24675609167478396], [0.47549507202220653, 0.6771774860822484, 0.30127868653789414]], [[0.6921865505936802, 0.32568285622231863, 0.8372953917074748], [0.09634549967404893, 0.34400668901519726, 0.1708820837424575], [0.008407132441445975, 0.8772648858897613, 0.4418820369801867]]], [[[0.07272107417893392, 0.6547653366163484, 0.6162370832047006], [0.6592090506246859, 0.8077681306483302, 0.3722609936257302], [0.885624749770039, 0.07557697407387987, 0.8549666072693299]], [[0.9410088543894836, 0.410534931700283, 0.21764379165311376], [0.4178900460272047, 0.5285665340425396, 0.7319151476775078], [0.5681970311880645, 0.12368329401649447, 0.41531170982297627]], [[0.6983613505215065, 0.8222892819044044, 0.5824201664722921], [0.8315076812420136, 0.7500642370845594, 0.452413617302191], [0.31207517492983705, 0.20212250471267257, 0.4502220619892995]]], [[[0.6929346246026855, 0.37813054055054207, 0.648167838489127], [0.6029414613166421, 0.19518745596916398, 0.8485314638199509], [0.774260144784503, 0.028216020222023297, 0.3875243178482526]], [[0.9371095900616052, 0.33052007302365805, 0.47443649643571384], [0.37461811674859136, 0.754183575715088, 0.2112467791954069], [0.7565416237998699, 0.9589576740110078, 0.4257862556618066]], [[0.2016762967663316, 0.8755295922412538, 0.5336175321372779], [0.5480234740935869, 0.973597247958125, 0.4088895361870102], [0.5984936048931023, 0.04220037156388723, 0.917493431404561]]], [[[0.6459341697148939, 0.890983652283764, 0.46225393886908395], [0.061074486499374636, 0.7097770085604174, 0.7842885742671676], [0.9857003680845567, 0.3948323671657289, 0.9231495207080516]], [[0.7834882135987349, 0.6651095050293149, 0.9350586369334877], [0.2494338901387405, 0.5845526138064847, 0.365363433321677], [0.9419437880704484, 0.10210846852691158, 0.10057858999692948]], [[0.0817621302692556, 0.5151115983112127, 0.9377342404046386], [0.1455547970391673, 0.3110333460929512, 0.40786246860256337], [0.9338786393796107, 0.7776271705439652, 0.9761620727539859]]]]\n",
            "idx=1, down_0.bias ==> (torch.float64, torch.Size([16]))\n",
            "ckpt idx 1 , weight = [0.8607956363677708, 0.20116912451437408, 0.2094873974783037, 0.007215689564179395, 0.9004490345282495, 0.03615270655783387, 0.3312630259684528, 0.5822982648219786, 0.08428623790403578, 0.28405527552668164, 0.13049023189062348, 0.2989169576838029, 0.025162372670322508, 0.892164893740192, 0.13101002860460698, 0.9116132733399909]\n",
            "idx=2, down_1.0.weight ==> (torch.float64, torch.Size([16, 16, 4, 4]))\n",
            "idx=3, down_1.0.bias ==> (torch.float64, torch.Size([16]))\n",
            "idx=4, down_1.2.weight ==> (torch.float64, torch.Size([16, 16, 3, 3]))\n",
            "idx=5, down_1.2.bias ==> (torch.float64, torch.Size([16]))\n",
            "idx=6, down_2.0.weight ==> (torch.float64, torch.Size([24, 16, 4, 4]))\n",
            "idx=7, down_2.0.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=8, down_2.2.weight ==> (torch.float64, torch.Size([24, 24, 3, 3]))\n",
            "idx=9, down_2.2.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=10, down_3.0.weight ==> (torch.float64, torch.Size([24, 24, 4, 4]))\n",
            "idx=11, down_3.0.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=12, down_3.2.weight ==> (torch.float64, torch.Size([24, 24, 3, 3]))\n",
            "idx=13, down_3.2.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=14, down_4.0.weight ==> (torch.float64, torch.Size([32, 24, 4, 4]))\n",
            "idx=15, down_4.0.bias ==> (torch.float64, torch.Size([32]))\n",
            "idx=16, down_4.2.weight ==> (torch.float64, torch.Size([32, 32, 3, 3]))\n",
            "idx=17, down_4.2.bias ==> (torch.float64, torch.Size([32]))\n",
            "idx=18, down_4.4.weight ==> (torch.float64, torch.Size([32, 32, 3, 3]))\n",
            "idx=19, down_4.4.bias ==> (torch.float64, torch.Size([32]))\n",
            "idx=20, down_4.6.weight ==> (torch.float64, torch.Size([32, 32, 3, 3]))\n",
            "idx=21, down_4.6.bias ==> (torch.float64, torch.Size([32]))\n",
            "idx=22, up_3.up_conv.0.weight ==> (torch.float64, torch.Size([32, 24, 2, 2]))\n",
            "idx=23, up_3.up_conv.0.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=24, up_3.merge_conv.0.weight ==> (torch.float64, torch.Size([24, 48, 1, 1]))\n",
            "idx=25, up_3.merge_conv.0.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=26, up_3.merge_conv.2.weight ==> (torch.float64, torch.Size([24, 24, 3, 3]))\n",
            "idx=27, up_3.merge_conv.2.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=28, up_3.merge_conv.4.weight ==> (torch.float64, torch.Size([24, 24, 3, 3]))\n",
            "idx=29, up_3.merge_conv.4.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=30, up_2.up_conv.0.weight ==> (torch.float64, torch.Size([24, 24, 2, 2]))\n",
            "idx=31, up_2.up_conv.0.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=32, up_2.merge_conv.0.weight ==> (torch.float64, torch.Size([24, 48, 1, 1]))\n",
            "idx=33, up_2.merge_conv.0.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=34, up_2.merge_conv.2.weight ==> (torch.float64, torch.Size([24, 24, 3, 3]))\n",
            "idx=35, up_2.merge_conv.2.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=36, up_2.merge_conv.4.weight ==> (torch.float64, torch.Size([24, 24, 3, 3]))\n",
            "idx=37, up_2.merge_conv.4.bias ==> (torch.float64, torch.Size([24]))\n",
            "idx=38, up_1.up_conv.0.weight ==> (torch.float64, torch.Size([24, 16, 2, 2]))\n",
            "idx=39, up_1.up_conv.0.bias ==> (torch.float64, torch.Size([16]))\n",
            "idx=40, up_1.merge_conv.0.weight ==> (torch.float64, torch.Size([16, 32, 1, 1]))\n",
            "idx=41, up_1.merge_conv.0.bias ==> (torch.float64, torch.Size([16]))\n",
            "idx=42, up_1.merge_conv.2.weight ==> (torch.float64, torch.Size([16, 16, 3, 3]))\n",
            "idx=43, up_1.merge_conv.2.bias ==> (torch.float64, torch.Size([16]))\n",
            "idx=44, up_1.merge_conv.4.weight ==> (torch.float64, torch.Size([16, 16, 3, 3]))\n",
            "idx=45, up_1.merge_conv.4.bias ==> (torch.float64, torch.Size([16]))\n",
            "idx=46, up_0.up_conv.0.weight ==> (torch.float64, torch.Size([16, 16, 2, 2]))\n",
            "idx=47, up_0.up_conv.0.bias ==> (torch.float64, torch.Size([16]))\n",
            "idx=48, up_0.merge_conv.0.weight ==> (torch.float64, torch.Size([16, 32, 1, 1]))\n",
            "idx=49, up_0.merge_conv.0.bias ==> (torch.float64, torch.Size([16]))\n",
            "idx=50, up_0.merge_conv.2.weight ==> (torch.float64, torch.Size([16, 16, 3, 3]))\n",
            "idx=51, up_0.merge_conv.2.bias ==> (torch.float64, torch.Size([16]))\n",
            "idx=52, up_0.merge_conv.4.weight ==> (torch.float64, torch.Size([16, 16, 3, 3]))\n",
            "idx=53, up_0.merge_conv.4.bias ==> (torch.float64, torch.Size([16]))\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch weight from torch model\n",
        "torch_weights_json = json_file\n",
        "with open(torch_weights_json, 'r') as f:\n",
        "    dict_from_json = json.load(f)\n",
        "if 1:\n",
        "  # Convert lists back to numpy arrays\n",
        "  torch_weights = {k: np.array(v) for k, v in dict_from_json.items()}\n",
        "else:\n",
        "  # read memory data\n",
        "  torch_weights = {k: v.numpy() for k, v in ckpt.items()}\n",
        "\n",
        "if 1:\n",
        "    n1 = 0\n",
        "    for k,v in torch_weights.items():\n",
        "        print (f\"idx={n1}, torch, {k} ==> ({v.dtype}, {v.shape})\")\n",
        "        n1 += 1\n",
        "\n",
        "    n2 = 0\n",
        "    for w in feature_extractor.weights:\n",
        "        print (f\"idx={n2} ==> tensorflow, {w.name}: ({w.dtype}, {w.shape})\")\n",
        "        n2 += 1\n",
        "    print (f\"n1={n1}, n2={n2}\")\n",
        "\n",
        "keras_weights = []\n",
        "for i, (k,v) in enumerate(torch_weights.items()):\n",
        "    if k.endswith('.weight'):\n",
        "        ## conv2d layer: Torch (out,in,h,w) vs Keras (h,w,in,out)\n",
        "        w = np.transpose(v, (2,3,1,0))\n",
        "    elif k.endswith('.bias'):\n",
        "        w = v\n",
        "    #keras_weights.append(np.ascontiguousarray(w))\n",
        "    keras_weights.append(w)\n",
        "    #if i <= 1:\n",
        "    #  print (f\"idx {i}, w = {w}\")\n",
        "    if 'init_layer_0.conv_em.bias' == k:\n",
        "        # repeat the above two again for new self.conv_em_right in the TF model;\n",
        "        w1 = np.transpose(\n",
        "            torch_weights['init_layer_0.conv_em.weight'], (2,3,1,0)\n",
        "            )\n",
        "        b1 = torch_weights['init_layer_0.conv_em.bias']\n",
        "        keras_weights.append(np.ascontiguousarray(w1))\n",
        "        keras_weights.append(b1)\n",
        "assert len(keras_weights) == len(feature_extractor.weights)\n",
        "\n",
        "feature_extractor.set_weights(keras_weights)\n",
        "lf2 = feature_extractor(x_tf)\n",
        "\n",
        "\n",
        "# check the model weights\n",
        "tf_weights_loaded = feature_extractor.get_weights()\n",
        "for i, (k,v) in enumerate(torch_weights.items()):\n",
        "  tf_w = tf_weights_loaded[i]\n",
        "  ke_w = keras_weights[i]\n",
        "  eql1 = np.allclose( tf_w, ke_w)\n",
        "  if k.endswith('.weight'):\n",
        "    ## conv2d layer: Torch (out,in,h,w) vs Keras (h,w,in,out)\n",
        "    pt_w = np.transpose(v, (2,3,1,0))\n",
        "  elif k.endswith('.bias'):\n",
        "    pt_w = v\n",
        "  eql2 = np.allclose(tf_w, pt_w)\n",
        "  print (f\"idx {i}, dtype = {tf_w.dtype}, {pt_w.dtype}, equal or not = {eql1}, {eql2}\")\n",
        "\n",
        "#pdb.set_trace()\n",
        "# Set to Keras model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VsySY_2D5TO",
        "outputId": "c444c289-0020-42d9-bf2e-5d00533236b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx=0, torch, down_0.weight ==> (float64, (16, 3, 3, 3))\n",
            "idx=1, torch, down_0.bias ==> (float64, (16,))\n",
            "idx=2, torch, down_1.0.weight ==> (float64, (16, 16, 4, 4))\n",
            "idx=3, torch, down_1.0.bias ==> (float64, (16,))\n",
            "idx=4, torch, down_1.2.weight ==> (float64, (16, 16, 3, 3))\n",
            "idx=5, torch, down_1.2.bias ==> (float64, (16,))\n",
            "idx=6, torch, down_2.0.weight ==> (float64, (24, 16, 4, 4))\n",
            "idx=7, torch, down_2.0.bias ==> (float64, (24,))\n",
            "idx=8, torch, down_2.2.weight ==> (float64, (24, 24, 3, 3))\n",
            "idx=9, torch, down_2.2.bias ==> (float64, (24,))\n",
            "idx=10, torch, down_3.0.weight ==> (float64, (24, 24, 4, 4))\n",
            "idx=11, torch, down_3.0.bias ==> (float64, (24,))\n",
            "idx=12, torch, down_3.2.weight ==> (float64, (24, 24, 3, 3))\n",
            "idx=13, torch, down_3.2.bias ==> (float64, (24,))\n",
            "idx=14, torch, down_4.0.weight ==> (float64, (32, 24, 4, 4))\n",
            "idx=15, torch, down_4.0.bias ==> (float64, (32,))\n",
            "idx=16, torch, down_4.2.weight ==> (float64, (32, 32, 3, 3))\n",
            "idx=17, torch, down_4.2.bias ==> (float64, (32,))\n",
            "idx=18, torch, down_4.4.weight ==> (float64, (32, 32, 3, 3))\n",
            "idx=19, torch, down_4.4.bias ==> (float64, (32,))\n",
            "idx=20, torch, down_4.6.weight ==> (float64, (32, 32, 3, 3))\n",
            "idx=21, torch, down_4.6.bias ==> (float64, (32,))\n",
            "idx=22, torch, up_3.up_conv.0.weight ==> (float64, (32, 24, 2, 2))\n",
            "idx=23, torch, up_3.up_conv.0.bias ==> (float64, (24,))\n",
            "idx=24, torch, up_3.merge_conv.0.weight ==> (float64, (24, 48, 1, 1))\n",
            "idx=25, torch, up_3.merge_conv.0.bias ==> (float64, (24,))\n",
            "idx=26, torch, up_3.merge_conv.2.weight ==> (float64, (24, 24, 3, 3))\n",
            "idx=27, torch, up_3.merge_conv.2.bias ==> (float64, (24,))\n",
            "idx=28, torch, up_3.merge_conv.4.weight ==> (float64, (24, 24, 3, 3))\n",
            "idx=29, torch, up_3.merge_conv.4.bias ==> (float64, (24,))\n",
            "idx=30, torch, up_2.up_conv.0.weight ==> (float64, (24, 24, 2, 2))\n",
            "idx=31, torch, up_2.up_conv.0.bias ==> (float64, (24,))\n",
            "idx=32, torch, up_2.merge_conv.0.weight ==> (float64, (24, 48, 1, 1))\n",
            "idx=33, torch, up_2.merge_conv.0.bias ==> (float64, (24,))\n",
            "idx=34, torch, up_2.merge_conv.2.weight ==> (float64, (24, 24, 3, 3))\n",
            "idx=35, torch, up_2.merge_conv.2.bias ==> (float64, (24,))\n",
            "idx=36, torch, up_2.merge_conv.4.weight ==> (float64, (24, 24, 3, 3))\n",
            "idx=37, torch, up_2.merge_conv.4.bias ==> (float64, (24,))\n",
            "idx=38, torch, up_1.up_conv.0.weight ==> (float64, (24, 16, 2, 2))\n",
            "idx=39, torch, up_1.up_conv.0.bias ==> (float64, (16,))\n",
            "idx=40, torch, up_1.merge_conv.0.weight ==> (float64, (16, 32, 1, 1))\n",
            "idx=41, torch, up_1.merge_conv.0.bias ==> (float64, (16,))\n",
            "idx=42, torch, up_1.merge_conv.2.weight ==> (float64, (16, 16, 3, 3))\n",
            "idx=43, torch, up_1.merge_conv.2.bias ==> (float64, (16,))\n",
            "idx=44, torch, up_1.merge_conv.4.weight ==> (float64, (16, 16, 3, 3))\n",
            "idx=45, torch, up_1.merge_conv.4.bias ==> (float64, (16,))\n",
            "idx=46, torch, up_0.up_conv.0.weight ==> (float64, (16, 16, 2, 2))\n",
            "idx=47, torch, up_0.up_conv.0.bias ==> (float64, (16,))\n",
            "idx=48, torch, up_0.merge_conv.0.weight ==> (float64, (16, 32, 1, 1))\n",
            "idx=49, torch, up_0.merge_conv.0.bias ==> (float64, (16,))\n",
            "idx=50, torch, up_0.merge_conv.2.weight ==> (float64, (16, 16, 3, 3))\n",
            "idx=51, torch, up_0.merge_conv.2.bias ==> (float64, (16,))\n",
            "idx=52, torch, up_0.merge_conv.4.weight ==> (float64, (16, 16, 3, 3))\n",
            "idx=53, torch, up_0.merge_conv.4.bias ==> (float64, (16,))\n",
            "idx=0 ==> tensorflow, feature_extractor/conv2d/kernel:0: (<dtype: 'float64'>, (3, 3, 3, 16))\n",
            "idx=1 ==> tensorflow, feature_extractor/conv2d/bias:0: (<dtype: 'float64'>, (16,))\n",
            "idx=2 ==> tensorflow, conv2d_1/kernel:0: (<dtype: 'float64'>, (4, 4, 16, 16))\n",
            "idx=3 ==> tensorflow, conv2d_1/bias:0: (<dtype: 'float64'>, (16,))\n",
            "idx=4 ==> tensorflow, conv2d_2/kernel:0: (<dtype: 'float64'>, (3, 3, 16, 16))\n",
            "idx=5 ==> tensorflow, conv2d_2/bias:0: (<dtype: 'float64'>, (16,))\n",
            "idx=6 ==> tensorflow, conv2d_3/kernel:0: (<dtype: 'float64'>, (4, 4, 16, 24))\n",
            "idx=7 ==> tensorflow, conv2d_3/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=8 ==> tensorflow, conv2d_4/kernel:0: (<dtype: 'float64'>, (3, 3, 24, 24))\n",
            "idx=9 ==> tensorflow, conv2d_4/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=10 ==> tensorflow, conv2d_5/kernel:0: (<dtype: 'float64'>, (4, 4, 24, 24))\n",
            "idx=11 ==> tensorflow, conv2d_5/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=12 ==> tensorflow, conv2d_6/kernel:0: (<dtype: 'float64'>, (3, 3, 24, 24))\n",
            "idx=13 ==> tensorflow, conv2d_6/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=14 ==> tensorflow, conv2d_7/kernel:0: (<dtype: 'float64'>, (4, 4, 24, 32))\n",
            "idx=15 ==> tensorflow, conv2d_7/bias:0: (<dtype: 'float64'>, (32,))\n",
            "idx=16 ==> tensorflow, conv2d_8/kernel:0: (<dtype: 'float64'>, (3, 3, 32, 32))\n",
            "idx=17 ==> tensorflow, conv2d_8/bias:0: (<dtype: 'float64'>, (32,))\n",
            "idx=18 ==> tensorflow, conv2d_9/kernel:0: (<dtype: 'float64'>, (3, 3, 32, 32))\n",
            "idx=19 ==> tensorflow, conv2d_9/bias:0: (<dtype: 'float64'>, (32,))\n",
            "idx=20 ==> tensorflow, conv2d_10/kernel:0: (<dtype: 'float64'>, (3, 3, 32, 32))\n",
            "idx=21 ==> tensorflow, conv2d_10/bias:0: (<dtype: 'float64'>, (32,))\n",
            "idx=22 ==> tensorflow, conv2d_transpose/kernel:0: (<dtype: 'float64'>, (2, 2, 24, 32))\n",
            "idx=23 ==> tensorflow, conv2d_transpose/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=24 ==> tensorflow, conv2d_11/kernel:0: (<dtype: 'float64'>, (1, 1, 48, 24))\n",
            "idx=25 ==> tensorflow, conv2d_11/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=26 ==> tensorflow, conv2d_12/kernel:0: (<dtype: 'float64'>, (3, 3, 24, 24))\n",
            "idx=27 ==> tensorflow, conv2d_12/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=28 ==> tensorflow, conv2d_13/kernel:0: (<dtype: 'float64'>, (3, 3, 24, 24))\n",
            "idx=29 ==> tensorflow, conv2d_13/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=30 ==> tensorflow, conv2d_transpose_1/kernel:0: (<dtype: 'float64'>, (2, 2, 24, 24))\n",
            "idx=31 ==> tensorflow, conv2d_transpose_1/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=32 ==> tensorflow, conv2d_14/kernel:0: (<dtype: 'float64'>, (1, 1, 48, 24))\n",
            "idx=33 ==> tensorflow, conv2d_14/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=34 ==> tensorflow, conv2d_15/kernel:0: (<dtype: 'float64'>, (3, 3, 24, 24))\n",
            "idx=35 ==> tensorflow, conv2d_15/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=36 ==> tensorflow, conv2d_16/kernel:0: (<dtype: 'float64'>, (3, 3, 24, 24))\n",
            "idx=37 ==> tensorflow, conv2d_16/bias:0: (<dtype: 'float64'>, (24,))\n",
            "idx=38 ==> tensorflow, conv2d_transpose_2/kernel:0: (<dtype: 'float64'>, (2, 2, 16, 24))\n",
            "idx=39 ==> tensorflow, conv2d_transpose_2/bias:0: (<dtype: 'float64'>, (16,))\n",
            "idx=40 ==> tensorflow, conv2d_17/kernel:0: (<dtype: 'float64'>, (1, 1, 32, 16))\n",
            "idx=41 ==> tensorflow, conv2d_17/bias:0: (<dtype: 'float64'>, (16,))\n",
            "idx=42 ==> tensorflow, conv2d_18/kernel:0: (<dtype: 'float64'>, (3, 3, 16, 16))\n",
            "idx=43 ==> tensorflow, conv2d_18/bias:0: (<dtype: 'float64'>, (16,))\n",
            "idx=44 ==> tensorflow, conv2d_19/kernel:0: (<dtype: 'float64'>, (3, 3, 16, 16))\n",
            "idx=45 ==> tensorflow, conv2d_19/bias:0: (<dtype: 'float64'>, (16,))\n",
            "idx=46 ==> tensorflow, conv2d_transpose_3/kernel:0: (<dtype: 'float64'>, (2, 2, 16, 16))\n",
            "idx=47 ==> tensorflow, conv2d_transpose_3/bias:0: (<dtype: 'float64'>, (16,))\n",
            "idx=48 ==> tensorflow, conv2d_20/kernel:0: (<dtype: 'float64'>, (1, 1, 32, 16))\n",
            "idx=49 ==> tensorflow, conv2d_20/bias:0: (<dtype: 'float64'>, (16,))\n",
            "idx=50 ==> tensorflow, conv2d_21/kernel:0: (<dtype: 'float64'>, (3, 3, 16, 16))\n",
            "idx=51 ==> tensorflow, conv2d_21/bias:0: (<dtype: 'float64'>, (16,))\n",
            "idx=52 ==> tensorflow, conv2d_22/kernel:0: (<dtype: 'float64'>, (3, 3, 16, 16))\n",
            "idx=53 ==> tensorflow, conv2d_22/bias:0: (<dtype: 'float64'>, (16,))\n",
            "n1=54, n2=54\n",
            "tf x0 shape = (1, 128, 256, 16), some values = [[3.91926824 5.81627401 5.01474286 4.82458783 5.29005626]\n",
            " [6.21916518 7.65551395 7.66813092 6.18409405 6.82858778]\n",
            " [5.76677646 7.06619377 7.2765683  6.16707753 7.14074652]\n",
            " [4.88346984 6.88705355 6.03917426 8.72257558 7.32111244]\n",
            " [5.58896538 7.20987216 7.3621713  7.87393225 8.03545963]]\n",
            "(1, 8, 16, 32) 32\n",
            "(1, 16, 32, 24) 24\n",
            "(1, 32, 64, 24) 24\n",
            "(1, 64, 128, 16) 16\n",
            "idx 0, dtype = float64, float64, equal or not = True, True\n",
            "idx 1, dtype = float64, float64, equal or not = True, True\n",
            "idx 2, dtype = float64, float64, equal or not = True, True\n",
            "idx 3, dtype = float64, float64, equal or not = True, True\n",
            "idx 4, dtype = float64, float64, equal or not = True, True\n",
            "idx 5, dtype = float64, float64, equal or not = True, True\n",
            "idx 6, dtype = float64, float64, equal or not = True, True\n",
            "idx 7, dtype = float64, float64, equal or not = True, True\n",
            "idx 8, dtype = float64, float64, equal or not = True, True\n",
            "idx 9, dtype = float64, float64, equal or not = True, True\n",
            "idx 10, dtype = float64, float64, equal or not = True, True\n",
            "idx 11, dtype = float64, float64, equal or not = True, True\n",
            "idx 12, dtype = float64, float64, equal or not = True, True\n",
            "idx 13, dtype = float64, float64, equal or not = True, True\n",
            "idx 14, dtype = float64, float64, equal or not = True, True\n",
            "idx 15, dtype = float64, float64, equal or not = True, True\n",
            "idx 16, dtype = float64, float64, equal or not = True, True\n",
            "idx 17, dtype = float64, float64, equal or not = True, True\n",
            "idx 18, dtype = float64, float64, equal or not = True, True\n",
            "idx 19, dtype = float64, float64, equal or not = True, True\n",
            "idx 20, dtype = float64, float64, equal or not = True, True\n",
            "idx 21, dtype = float64, float64, equal or not = True, True\n",
            "idx 22, dtype = float64, float64, equal or not = True, True\n",
            "idx 23, dtype = float64, float64, equal or not = True, True\n",
            "idx 24, dtype = float64, float64, equal or not = True, True\n",
            "idx 25, dtype = float64, float64, equal or not = True, True\n",
            "idx 26, dtype = float64, float64, equal or not = True, True\n",
            "idx 27, dtype = float64, float64, equal or not = True, True\n",
            "idx 28, dtype = float64, float64, equal or not = True, True\n",
            "idx 29, dtype = float64, float64, equal or not = True, True\n",
            "idx 30, dtype = float64, float64, equal or not = True, True\n",
            "idx 31, dtype = float64, float64, equal or not = True, True\n",
            "idx 32, dtype = float64, float64, equal or not = True, True\n",
            "idx 33, dtype = float64, float64, equal or not = True, True\n",
            "idx 34, dtype = float64, float64, equal or not = True, True\n",
            "idx 35, dtype = float64, float64, equal or not = True, True\n",
            "idx 36, dtype = float64, float64, equal or not = True, True\n",
            "idx 37, dtype = float64, float64, equal or not = True, True\n",
            "idx 38, dtype = float64, float64, equal or not = True, True\n",
            "idx 39, dtype = float64, float64, equal or not = True, True\n",
            "idx 40, dtype = float64, float64, equal or not = True, True\n",
            "idx 41, dtype = float64, float64, equal or not = True, True\n",
            "idx 42, dtype = float64, float64, equal or not = True, True\n",
            "idx 43, dtype = float64, float64, equal or not = True, True\n",
            "idx 44, dtype = float64, float64, equal or not = True, True\n",
            "idx 45, dtype = float64, float64, equal or not = True, True\n",
            "idx 46, dtype = float64, float64, equal or not = True, True\n",
            "idx 47, dtype = float64, float64, equal or not = True, True\n",
            "idx 48, dtype = float64, float64, equal or not = True, True\n",
            "idx 49, dtype = float64, float64, equal or not = True, True\n",
            "idx 50, dtype = float64, float64, equal or not = True, True\n",
            "idx 51, dtype = float64, float64, equal or not = True, True\n",
            "idx 52, dtype = float64, float64, equal or not = True, True\n",
            "idx 53, dtype = float64, float64, equal or not = True, True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the results from PT and TF are the same or not. We use `np.allclose()` to check whether the tensors are close enough. If Not, we will check the details on which part are not equal."
      ],
      "metadata": {
        "id": "xZqJallGTSzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# Tensorflow output : lf2\n",
        "\n",
        "#PyTorch output : lf_pt\n",
        "\n",
        "is_verbose = False\n",
        "\n",
        "for my_atol in [1e-6, 1e-07, 1e-8]:\n",
        "    is_equal = all( np.allclose(lf2[k].numpy(), lf_pt[k].detach().numpy().transpose((0,2,3,1)), rtol=1e-05, atol=my_atol) for k in lf2.keys()\n",
        "            )\n",
        "    # Should print True if dictionaries are equal\n",
        "    print (\"\\n**********************\")\n",
        "    print (\"\\n**********************\")\n",
        "    print (f\"\\nSetting atol={my_atol}, the results from TensorFlow and PyTorch are equal or not = {is_equal}\")\n",
        "\n",
        "    if not is_equal:\n",
        "        for i, k in enumerate(lf2.keys()):\n",
        "          tf_y = lf2[k].numpy()\n",
        "          pt_y = lf_pt[k].detach().numpy()\n",
        "          pt_y2 = np.transpose(pt_y, (0, 2, 3, 1))\n",
        "          print (\"\\n------\")\n",
        "          print (f\"for feature {k} at shape {tf_y.shape}\")\n",
        "          #tmp1 = np.array_equal(pt_y2, tf_y)\n",
        "          tmp2 = np.allclose(pt_y2, tf_y, rtol=0, atol=my_atol)\n",
        "\n",
        "          tf_y_small = tf_y[0,:5,:5,0]\n",
        "          pt_y2_small = pt_y2[0,:5,:5,0]\n",
        "          print (f\"is_equal = {tmp2}\")\n",
        "          if is_verbose:\n",
        "            print (f\"is_equal = {tmp2}, \\n tf = \\n{tf_y_small}, \\n pt = \\n{pt_y2_small}, \\n diff = \\n{np.abs(tf_y_small - pt_y2_small)}\")\n",
        "          if k == 'x0':\n",
        "            #print (\"tf_y = \\n\", tf_y[0,:10,:10,0])\n",
        "            #print (\"pt_y = \\n\", pt_y2[0,:10,:10,0])\n",
        "            #for j in range(0,tf_y.shape[-1],2):\n",
        "            for j in range(0,1):\n",
        "              tmp_abs = np.abs(tf_y[0,:,:,j] - pt_y2[0,:,:,j])\n",
        "              print (f\" feature {k}: channl {j} has abs = {tmp_abs.mean()}\")\n",
        "              res = np.zeros((tf_y.shape[1], tf_y.shape[2], 3))\n",
        "              res[tmp_abs <= my_atol] = (0, 0, 0) # black\n",
        "              res[tmp_abs > my_atol] = (0, 0, 255) #red\n",
        "              print (f\"PS: black means diff <= thred {my_atol}, red means diff > thred {my_atol}\")\n",
        "              cv2_imshow(res.astype(np.uint8))\n",
        "            #sys.exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yjgwsc0HGZn",
        "outputId": "d67030ca-35b9-4ee8-b181-55163ac5518c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**********************\n",
            "\n",
            "**********************\n",
            "\n",
            "Setting atol=1e-06, the results from TensorFlow and PyTorch are equal or not = True\n",
            "\n",
            "**********************\n",
            "\n",
            "**********************\n",
            "\n",
            "Setting atol=1e-07, the results from TensorFlow and PyTorch are equal or not = True\n",
            "\n",
            "**********************\n",
            "\n",
            "**********************\n",
            "\n",
            "Setting atol=1e-08, the results from TensorFlow and PyTorch are equal or not = True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Toy Example for a Conv2D"
      ],
      "metadata": {
        "id": "WginuXn7dCWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# > see https://stackoverflow.com/questions/68165375/manualy-convert-pytorch-weights-to-tf-keras-weights-for-convolutional-layer;\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "in_plane = 3\n",
        "out_plane = 16\n",
        "k = 3\n",
        "s = 1\n",
        "p = 1\n",
        "\n",
        "h = 64\n",
        "w = 128\n",
        "\n",
        "tf_layer = tf.keras.layers.Conv2D(\n",
        "    filters=out_plane,\n",
        "    kernel_size=k,\n",
        "    strides=s,\n",
        "    padding='same',\n",
        "    use_bias=True\n",
        ")\n",
        "\n",
        "#setting weights in torch layer and tf layer respectively\n",
        "torch_weights = np.random.rand(out_plane, in_plane, k, k)\n",
        "tf_weights = np.transpose(torch_weights, (2, 3, 1, 0))\n",
        "bias = np.random.rand(out_plane)\n",
        "print (\"tf_weights shape \", tf_weights.shape, \" dtype = \", tf_weights.dtype)\n",
        "print (\"bias shape \", bias.shape, \" dtype = \", bias.dtype)\n",
        "\n",
        "#prepare inputs and do inference\n",
        "torch_input = np.random.rand(1, 3, h, w)\n",
        "tf_input = np.transpose(torch_input, (0, 2, 3, 1))\n",
        "\n",
        "res1 = tf_layer(tf_input)\n",
        "print(f\"res1 = {res1.shape}, \\n {res1[0,:5,:5,0]}\")\n",
        "\n",
        "tf_layer.set_weights([tf_weights, bias])\n",
        "res2 = tf_layer(tf_input)\n",
        "print(f\"res2 = {res2.shape}, \\n {res2[0,:5,:5,0]}\")\n",
        "\n",
        "\n",
        "\n",
        "#initialize the layers respectively\n",
        "torch_layer = torch.nn.Conv2d(\n",
        "    in_channels= in_plane,\n",
        "    out_channels= out_plane,\n",
        "    kernel_size= k,\n",
        "    stride= s,\n",
        "    padding = p,\n",
        "    bias = True\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "  torch_layer.weight = torch.nn.Parameter(torch.Tensor(torch_weights))\n",
        "  torch_layer.bias = torch.nn.Parameter(torch.Tensor(bias))\n",
        "\n",
        "torch_output = torch_layer(torch.Tensor(torch_input)).detach().numpy()\n",
        "print(f\"torch_output = {torch_output.shape}, \\n {torch_output[0,0,:5,:5]}\")\n",
        "diff = np.abs(res2.numpy() - np.transpose(torch_output, (0, 2, 3, 1)))\n",
        "print (\"err (mean) = \", diff.mean())\n",
        "\n",
        "\n",
        "rtol=1e-5\n",
        "atol=1e-8\n",
        "# NOTE: np.allclose condition is absolute(a - b) <= (atol + rtol * absolute(b))\n",
        "res1=np.allclose(res2.numpy(), np.transpose(torch_output, (0, 2, 3, 1)), rtol=rtol, atol=atol)\n",
        "print (\"-------\")\n",
        "print (f\"dtype = {tf_weights.dtype}, rtol={rtol}, atol={atol}: is_equal = {res1}\")\n",
        "\n",
        "rtol=0\n",
        "atol=1e-8\n",
        "res2=np.allclose(res2.numpy(), np.transpose(torch_output, (0, 2, 3, 1)), rtol=rtol, atol=atol)\n",
        "print (\"-------\")\n",
        "print (f\"dtype = {tf_weights.dtype}, rtol={rtol}, atol={atol}: is_equal = {res1}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41IWphAjmdd-",
        "outputId": "c9a28fa1-3986-4433-d3f4-cf1ec7bca598"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf_weights shape  (3, 3, 3, 16)  dtype =  float64\n",
            "bias shape  (16,)  dtype =  float64\n",
            "res1 = (1, 64, 128, 16), \n",
            " [[ 0.01783615 -0.19606026 -0.11291458  0.04376133 -0.17693851]\n",
            " [-0.24715062 -0.25317018 -0.34996817 -0.31757819 -0.51121735]\n",
            " [-0.27994044 -0.21161163 -0.41820707 -0.52221118 -0.22143423]\n",
            " [-0.03675212 -0.2639984  -0.42630367 -0.33346697 -0.08005202]\n",
            " [-0.29213373 -0.17761458 -0.25718101 -0.3077135  -0.17602881]]\n",
            "res2 = (1, 64, 128, 16), \n",
            " [[3.55552805 6.08036261 6.12241194 6.94787174 7.32713359]\n",
            " [5.12395157 7.62213585 8.8579661  9.90912449 9.14551106]\n",
            " [5.25447962 7.92904057 9.1406285  9.25985727 8.40454536]\n",
            " [5.79419285 8.55290875 8.17833809 8.38059463 8.7674977 ]\n",
            " [5.22129169 8.27632936 7.58890964 7.83379026 8.17289546]]\n",
            "torch_output = (1, 16, 64, 128), \n",
            " [[3.55552805 6.08036261 6.12241194 6.94787174 7.32713359]\n",
            " [5.12395157 7.62213585 8.8579661  9.90912449 9.14551106]\n",
            " [5.25447962 7.92904057 9.1406285  9.25985727 8.40454536]\n",
            " [5.79419285 8.55290875 8.17833809 8.38059463 8.7674977 ]\n",
            " [5.22129169 8.27632936 7.58890964 7.83379026 8.17289546]]\n",
            "err (mean) =  8.675514733685885e-16\n",
            "-------\n",
            "dtype = float64, rtol=1e-05, atol=1e-08: is_equal = True\n",
            "-------\n",
            "dtype = float64, rtol=0, atol=1e-08: is_equal = True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see: Pytorch convolution and tensorflow convolution giving different results\n",
        "# at https://discuss.pytorch.org/t/pytorch-convolution-and-tensorflow-convolution-giving-different-results/26863;\n",
        "import torch\n",
        "import tensorflow\n",
        "import numpy\n",
        "y = numpy.random.rand(1,100,100,1)\n",
        "filterx = numpy.random.rand(4,5,1,2)\n",
        "a= tensorflow.nn.conv2d(y, filterx, [1,1,1,1], 'VALID')\n",
        "\n",
        "\n",
        "x = torch.nn.Conv2d(1,1,5, bias = False)\n",
        "filter = numpy.transpose(filterx, (3,2,0,1))\n",
        "x.weight = torch.nn.Parameter(torch.from_numpy(filter))\n",
        "z = numpy.transpose(y, (0,3,1,2))\n",
        "l = x(torch.from_numpy(z))\n",
        "l = l.detach().numpy()\n",
        "l = numpy.transpose(l,(0,2,3,1))\n",
        "\n",
        "print(a.shape)\n",
        "print(l.shape)\n",
        "print(numpy.abs(a-l).max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mDJTREZ8eKX",
        "outputId": "d89071e1-e311-4888-fa17-be96d13d4389"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 97, 96, 2)\n",
            "(1, 97, 96, 2)\n",
            "3.552713678800501e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rerun the Network Using Float32\n",
        "\n",
        "Let us use `float32` and run the above code again!"
      ],
      "metadata": {
        "id": "u9B9uBWKEWl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USING_FLOAT64 = False\n",
        "\n",
        "if USING_FLOAT64:\n",
        "  tf.keras.backend.set_floatx('float64')\n",
        "  torch.set_default_dtype(torch.float64)\n",
        "  DATA_TYPE = np.float64\n",
        "else:\n",
        "  tf.keras.backend.set_floatx('float32')\n",
        "  torch.set_default_dtype(torch.float32)\n",
        "  DATA_TYPE = np.float32\n",
        "\n",
        "height = 128\n",
        "width = 256\n",
        "\n",
        "#prepare inputs and do inference\n",
        "x = np.random.rand(1, 3, height, width).astype(DATA_TYPE)\n",
        "x_tf = np.transpose(x, (0, 2, 3, 1)).astype(DATA_TYPE)\n",
        "\n",
        "\"\"\" # TF feature \"\"\"\n",
        "feature_extractor = FeatureExtractor(num_feature)\n",
        "lf = feature_extractor(x_tf)\n",
        "\n",
        "\"\"\" # PT code \"\"\"\n",
        "torch.set_printoptions(precision=8)\n",
        "feature_extractor_pt = FeatureExtractor_PT(num_feature)\n",
        "\n",
        "x_pt = torch.from_numpy(x)\n",
        "lf_pt = feature_extractor_pt(x_pt)\n",
        "\n",
        "conv0 = getattr(feature_extractor_pt, 'down_0')\n",
        "print (f\"\\nPyTorch ... conv0.weight.dtype = {conv0.weight.dtype}, conv0.bias.dtype={conv0.bias.dtype}\\n\")\n",
        "\n",
        "ckpt = feature_extractor_pt.state_dict()\n",
        "# Fetch weight from torch model by reading memory data\n",
        "torch_weights = {k: v.numpy() for k, v in ckpt.items()}\n",
        "\n",
        "if 1:\n",
        "    n1 = 0\n",
        "    for k,v in torch_weights.items():\n",
        "        print (f\"idx={n1}, torch, {k} ==> ({v.dtype}, {v.shape})\")\n",
        "        n1 += 1\n",
        "\n",
        "    n2 = 0\n",
        "    print (\"\\n\\n\")\n",
        "    for w in feature_extractor.weights:\n",
        "        print (f\"idx={n2} ==> tensorflow, {w.name}: ({w.dtype}, {w.shape})\")\n",
        "        n2 += 1\n",
        "    print (f\"n1={n1}, n2={n2}\")\n",
        "\n",
        "keras_weights = []\n",
        "for i, (k,v) in enumerate(torch_weights.items()):\n",
        "    if k.endswith('.weight'):\n",
        "        ## conv2d layer: Torch (out,in,h,w) vs Keras (h,w,in,out)\n",
        "        w = np.transpose(v, (2,3,1,0))\n",
        "    elif k.endswith('.bias'):\n",
        "        w = v\n",
        "    #keras_weights.append(np.ascontiguousarray(w))\n",
        "    keras_weights.append(w)\n",
        "    #if i <= 1:\n",
        "    #  print (f\"idx {i}, w = {w}\")\n",
        "    if 'init_layer_0.conv_em.bias' == k:\n",
        "        # repeat the above two again for new self.conv_em_right in the TF model;\n",
        "        w1 = np.transpose(\n",
        "            torch_weights['init_layer_0.conv_em.weight'], (2,3,1,0)\n",
        "            )\n",
        "        b1 = torch_weights['init_layer_0.conv_em.bias']\n",
        "        keras_weights.append(np.ascontiguousarray(w1))\n",
        "        keras_weights.append(b1)\n",
        "\n",
        "assert len(keras_weights) == len(feature_extractor.weights)\n",
        "\n",
        "feature_extractor.set_weights(keras_weights)\n",
        "lf2 = feature_extractor(x_tf)\n",
        "\n",
        "# check the model weights\n",
        "tf_weights_loaded = feature_extractor.get_weights()\n",
        "for i, (k,v) in enumerate(torch_weights.items()):\n",
        "  tf_w = tf_weights_loaded[i]\n",
        "  ke_w = keras_weights[i]\n",
        "  eql1 = np.allclose( tf_w, ke_w)\n",
        "  if k.endswith('.weight'):\n",
        "    ## conv2d layer: Torch (out,in,h,w) vs Keras (h,w,in,out)\n",
        "    pt_w = np.transpose(v, (2,3,1,0))\n",
        "  elif k.endswith('.bias'):\n",
        "    pt_w = v\n",
        "  eql2 = np.allclose(tf_w, pt_w)\n",
        "  print (f\"idx {i}, dtype = {tf_w.dtype}, {pt_w.dtype}, equal or not = {eql1}, {eql2}\")\n",
        "\n",
        "\n",
        "\"\"\" check the output at differnt threshold \"\"\"\n",
        "is_verbose = False\n",
        "for my_atol in [1e-6, 1e-07, 1e-8]:\n",
        "    is_equal = all( np.allclose(lf2[k].numpy(), lf_pt[k].detach().numpy().transpose((0,2,3,1)), rtol=1e-05, atol=my_atol) for k in lf2.keys()\n",
        "            )\n",
        "    # Should print True if dictionaries are equal\n",
        "    print (\"\\n**********************\")\n",
        "    print (\"\\n**********************\")\n",
        "    print (f\"\\nSetting atol={my_atol}, the results from TensorFlow and PyTorch are equal or not = {is_equal}\")\n",
        "\n",
        "    if not is_equal:\n",
        "        for i, k in enumerate(lf2.keys()):\n",
        "          tf_y = lf2[k].numpy()\n",
        "          pt_y = lf_pt[k].detach().numpy()\n",
        "          pt_y2 = np.transpose(pt_y, (0, 2, 3, 1))\n",
        "          print (\"\\n------\")\n",
        "          print (f\"for feature {k} at shape {tf_y.shape}\")\n",
        "          #tmp1 = np.array_equal(pt_y2, tf_y)\n",
        "          tmp2 = np.allclose(pt_y2, tf_y, rtol=0, atol=my_atol)\n",
        "\n",
        "          tf_y_small = tf_y[0,:5,:5,0]\n",
        "          pt_y2_small = pt_y2[0,:5,:5,0]\n",
        "          print (f\"is_equal = {tmp2}\")\n",
        "          if is_verbose:\n",
        "            print (f\"is_equal = {tmp2}, \\n tf = \\n{tf_y_small}, \\n pt = \\n{pt_y2_small}, \\n diff = \\n{np.abs(tf_y_small - pt_y2_small)}\")\n",
        "          if k == 'x0':\n",
        "            #print (\"tf_y = \\n\", tf_y[0,:10,:10,0])\n",
        "            #print (\"pt_y = \\n\", pt_y2[0,:10,:10,0])\n",
        "            #for j in range(0,tf_y.shape[-1],2):\n",
        "            for j in range(0,1):\n",
        "              tmp_abs = np.abs(tf_y[0,:,:,j] - pt_y2[0,:,:,j])\n",
        "              print (f\" feature {k}: channl {j} has abs = {tmp_abs.mean()}\")\n",
        "              res = np.zeros((tf_y.shape[1], tf_y.shape[2], 3))\n",
        "              res[tmp_abs <= my_atol] = (0, 0, 0) # black\n",
        "              res[tmp_abs > my_atol] = (0, 0, 255) #red\n",
        "              print (f\"PS: black means diff <= thred {my_atol}, red means diff > thred {my_atol}\")\n",
        "              cv2_imshow(res.astype(np.uint8))\n",
        "            #sys.exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iZLzkp36acXI",
        "outputId": "bb56f2af-9168-4f1c-85e0-cbee4755ccde"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf x0 shape = (1, 128, 256, 16), some values = [[-0.10128167 -0.34591514 -0.16692868 -0.40104696 -0.18563092]\n",
            " [-0.32168305 -0.6819552  -0.49588716 -0.44792494 -0.56712127]\n",
            " [-0.64002985 -0.53255254 -0.5417126  -0.13429967 -0.6559283 ]\n",
            " [-0.34781545 -0.61244476 -0.7748004  -0.33628228 -0.48780736]\n",
            " [-0.19926704 -0.5531465  -0.45617053 -0.41027695 -0.46094066]]\n",
            "(1, 8, 16, 32) 32\n",
            "(1, 16, 32, 24) 24\n",
            "(1, 32, 64, 24) 24\n",
            "(1, 64, 128, 16) 16\n",
            "Pytorch x0 shape = torch.Size([1, 16, 128, 256]), some values = tensor([[-0.45660520, -0.28543013, -0.21115358, -0.12227429, -0.46161559],\n",
            "        [-0.46589008, -0.16889663, -0.28308401, -0.04657911, -0.48355237],\n",
            "        [-0.05238248, -0.08155582, -0.15507492, -0.40920278, -0.23588191],\n",
            "        [-0.03442940, -0.08689006, -0.21321033, -0.46286422,  0.13467410],\n",
            "        [-0.13919899, -0.07587539, -0.32083479, -0.12369461,  0.20571502]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "PyTorch ... conv0.weight.dtype = torch.float32, conv0.bias.dtype=torch.float32\n",
            "\n",
            "idx=0, torch, down_0.weight ==> (float32, (16, 3, 3, 3))\n",
            "idx=1, torch, down_0.bias ==> (float32, (16,))\n",
            "idx=2, torch, down_1.0.weight ==> (float32, (16, 16, 4, 4))\n",
            "idx=3, torch, down_1.0.bias ==> (float32, (16,))\n",
            "idx=4, torch, down_1.2.weight ==> (float32, (16, 16, 3, 3))\n",
            "idx=5, torch, down_1.2.bias ==> (float32, (16,))\n",
            "idx=6, torch, down_2.0.weight ==> (float32, (24, 16, 4, 4))\n",
            "idx=7, torch, down_2.0.bias ==> (float32, (24,))\n",
            "idx=8, torch, down_2.2.weight ==> (float32, (24, 24, 3, 3))\n",
            "idx=9, torch, down_2.2.bias ==> (float32, (24,))\n",
            "idx=10, torch, down_3.0.weight ==> (float32, (24, 24, 4, 4))\n",
            "idx=11, torch, down_3.0.bias ==> (float32, (24,))\n",
            "idx=12, torch, down_3.2.weight ==> (float32, (24, 24, 3, 3))\n",
            "idx=13, torch, down_3.2.bias ==> (float32, (24,))\n",
            "idx=14, torch, down_4.0.weight ==> (float32, (32, 24, 4, 4))\n",
            "idx=15, torch, down_4.0.bias ==> (float32, (32,))\n",
            "idx=16, torch, down_4.2.weight ==> (float32, (32, 32, 3, 3))\n",
            "idx=17, torch, down_4.2.bias ==> (float32, (32,))\n",
            "idx=18, torch, down_4.4.weight ==> (float32, (32, 32, 3, 3))\n",
            "idx=19, torch, down_4.4.bias ==> (float32, (32,))\n",
            "idx=20, torch, down_4.6.weight ==> (float32, (32, 32, 3, 3))\n",
            "idx=21, torch, down_4.6.bias ==> (float32, (32,))\n",
            "idx=22, torch, up_3.up_conv.0.weight ==> (float32, (32, 24, 2, 2))\n",
            "idx=23, torch, up_3.up_conv.0.bias ==> (float32, (24,))\n",
            "idx=24, torch, up_3.merge_conv.0.weight ==> (float32, (24, 48, 1, 1))\n",
            "idx=25, torch, up_3.merge_conv.0.bias ==> (float32, (24,))\n",
            "idx=26, torch, up_3.merge_conv.2.weight ==> (float32, (24, 24, 3, 3))\n",
            "idx=27, torch, up_3.merge_conv.2.bias ==> (float32, (24,))\n",
            "idx=28, torch, up_3.merge_conv.4.weight ==> (float32, (24, 24, 3, 3))\n",
            "idx=29, torch, up_3.merge_conv.4.bias ==> (float32, (24,))\n",
            "idx=30, torch, up_2.up_conv.0.weight ==> (float32, (24, 24, 2, 2))\n",
            "idx=31, torch, up_2.up_conv.0.bias ==> (float32, (24,))\n",
            "idx=32, torch, up_2.merge_conv.0.weight ==> (float32, (24, 48, 1, 1))\n",
            "idx=33, torch, up_2.merge_conv.0.bias ==> (float32, (24,))\n",
            "idx=34, torch, up_2.merge_conv.2.weight ==> (float32, (24, 24, 3, 3))\n",
            "idx=35, torch, up_2.merge_conv.2.bias ==> (float32, (24,))\n",
            "idx=36, torch, up_2.merge_conv.4.weight ==> (float32, (24, 24, 3, 3))\n",
            "idx=37, torch, up_2.merge_conv.4.bias ==> (float32, (24,))\n",
            "idx=38, torch, up_1.up_conv.0.weight ==> (float32, (24, 16, 2, 2))\n",
            "idx=39, torch, up_1.up_conv.0.bias ==> (float32, (16,))\n",
            "idx=40, torch, up_1.merge_conv.0.weight ==> (float32, (16, 32, 1, 1))\n",
            "idx=41, torch, up_1.merge_conv.0.bias ==> (float32, (16,))\n",
            "idx=42, torch, up_1.merge_conv.2.weight ==> (float32, (16, 16, 3, 3))\n",
            "idx=43, torch, up_1.merge_conv.2.bias ==> (float32, (16,))\n",
            "idx=44, torch, up_1.merge_conv.4.weight ==> (float32, (16, 16, 3, 3))\n",
            "idx=45, torch, up_1.merge_conv.4.bias ==> (float32, (16,))\n",
            "idx=46, torch, up_0.up_conv.0.weight ==> (float32, (16, 16, 2, 2))\n",
            "idx=47, torch, up_0.up_conv.0.bias ==> (float32, (16,))\n",
            "idx=48, torch, up_0.merge_conv.0.weight ==> (float32, (16, 32, 1, 1))\n",
            "idx=49, torch, up_0.merge_conv.0.bias ==> (float32, (16,))\n",
            "idx=50, torch, up_0.merge_conv.2.weight ==> (float32, (16, 16, 3, 3))\n",
            "idx=51, torch, up_0.merge_conv.2.bias ==> (float32, (16,))\n",
            "idx=52, torch, up_0.merge_conv.4.weight ==> (float32, (16, 16, 3, 3))\n",
            "idx=53, torch, up_0.merge_conv.4.bias ==> (float32, (16,))\n",
            "\n",
            "\n",
            "\n",
            "idx=0 ==> tensorflow, feature_extractor_5/conv2d_118/kernel:0: (<dtype: 'float32'>, (3, 3, 3, 16))\n",
            "idx=1 ==> tensorflow, feature_extractor_5/conv2d_118/bias:0: (<dtype: 'float32'>, (16,))\n",
            "idx=2 ==> tensorflow, conv2d_119/kernel:0: (<dtype: 'float32'>, (4, 4, 16, 16))\n",
            "idx=3 ==> tensorflow, conv2d_119/bias:0: (<dtype: 'float32'>, (16,))\n",
            "idx=4 ==> tensorflow, conv2d_120/kernel:0: (<dtype: 'float32'>, (3, 3, 16, 16))\n",
            "idx=5 ==> tensorflow, conv2d_120/bias:0: (<dtype: 'float32'>, (16,))\n",
            "idx=6 ==> tensorflow, conv2d_121/kernel:0: (<dtype: 'float32'>, (4, 4, 16, 24))\n",
            "idx=7 ==> tensorflow, conv2d_121/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=8 ==> tensorflow, conv2d_122/kernel:0: (<dtype: 'float32'>, (3, 3, 24, 24))\n",
            "idx=9 ==> tensorflow, conv2d_122/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=10 ==> tensorflow, conv2d_123/kernel:0: (<dtype: 'float32'>, (4, 4, 24, 24))\n",
            "idx=11 ==> tensorflow, conv2d_123/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=12 ==> tensorflow, conv2d_124/kernel:0: (<dtype: 'float32'>, (3, 3, 24, 24))\n",
            "idx=13 ==> tensorflow, conv2d_124/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=14 ==> tensorflow, conv2d_125/kernel:0: (<dtype: 'float32'>, (4, 4, 24, 32))\n",
            "idx=15 ==> tensorflow, conv2d_125/bias:0: (<dtype: 'float32'>, (32,))\n",
            "idx=16 ==> tensorflow, conv2d_126/kernel:0: (<dtype: 'float32'>, (3, 3, 32, 32))\n",
            "idx=17 ==> tensorflow, conv2d_126/bias:0: (<dtype: 'float32'>, (32,))\n",
            "idx=18 ==> tensorflow, conv2d_127/kernel:0: (<dtype: 'float32'>, (3, 3, 32, 32))\n",
            "idx=19 ==> tensorflow, conv2d_127/bias:0: (<dtype: 'float32'>, (32,))\n",
            "idx=20 ==> tensorflow, conv2d_128/kernel:0: (<dtype: 'float32'>, (3, 3, 32, 32))\n",
            "idx=21 ==> tensorflow, conv2d_128/bias:0: (<dtype: 'float32'>, (32,))\n",
            "idx=22 ==> tensorflow, conv2d_transpose_20/kernel:0: (<dtype: 'float32'>, (2, 2, 24, 32))\n",
            "idx=23 ==> tensorflow, conv2d_transpose_20/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=24 ==> tensorflow, conv2d_129/kernel:0: (<dtype: 'float32'>, (1, 1, 48, 24))\n",
            "idx=25 ==> tensorflow, conv2d_129/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=26 ==> tensorflow, conv2d_130/kernel:0: (<dtype: 'float32'>, (3, 3, 24, 24))\n",
            "idx=27 ==> tensorflow, conv2d_130/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=28 ==> tensorflow, conv2d_131/kernel:0: (<dtype: 'float32'>, (3, 3, 24, 24))\n",
            "idx=29 ==> tensorflow, conv2d_131/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=30 ==> tensorflow, conv2d_transpose_21/kernel:0: (<dtype: 'float32'>, (2, 2, 24, 24))\n",
            "idx=31 ==> tensorflow, conv2d_transpose_21/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=32 ==> tensorflow, conv2d_132/kernel:0: (<dtype: 'float32'>, (1, 1, 48, 24))\n",
            "idx=33 ==> tensorflow, conv2d_132/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=34 ==> tensorflow, conv2d_133/kernel:0: (<dtype: 'float32'>, (3, 3, 24, 24))\n",
            "idx=35 ==> tensorflow, conv2d_133/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=36 ==> tensorflow, conv2d_134/kernel:0: (<dtype: 'float32'>, (3, 3, 24, 24))\n",
            "idx=37 ==> tensorflow, conv2d_134/bias:0: (<dtype: 'float32'>, (24,))\n",
            "idx=38 ==> tensorflow, conv2d_transpose_22/kernel:0: (<dtype: 'float32'>, (2, 2, 16, 24))\n",
            "idx=39 ==> tensorflow, conv2d_transpose_22/bias:0: (<dtype: 'float32'>, (16,))\n",
            "idx=40 ==> tensorflow, conv2d_135/kernel:0: (<dtype: 'float32'>, (1, 1, 32, 16))\n",
            "idx=41 ==> tensorflow, conv2d_135/bias:0: (<dtype: 'float32'>, (16,))\n",
            "idx=42 ==> tensorflow, conv2d_136/kernel:0: (<dtype: 'float32'>, (3, 3, 16, 16))\n",
            "idx=43 ==> tensorflow, conv2d_136/bias:0: (<dtype: 'float32'>, (16,))\n",
            "idx=44 ==> tensorflow, conv2d_137/kernel:0: (<dtype: 'float32'>, (3, 3, 16, 16))\n",
            "idx=45 ==> tensorflow, conv2d_137/bias:0: (<dtype: 'float32'>, (16,))\n",
            "idx=46 ==> tensorflow, conv2d_transpose_23/kernel:0: (<dtype: 'float32'>, (2, 2, 16, 16))\n",
            "idx=47 ==> tensorflow, conv2d_transpose_23/bias:0: (<dtype: 'float32'>, (16,))\n",
            "idx=48 ==> tensorflow, conv2d_138/kernel:0: (<dtype: 'float32'>, (1, 1, 32, 16))\n",
            "idx=49 ==> tensorflow, conv2d_138/bias:0: (<dtype: 'float32'>, (16,))\n",
            "idx=50 ==> tensorflow, conv2d_139/kernel:0: (<dtype: 'float32'>, (3, 3, 16, 16))\n",
            "idx=51 ==> tensorflow, conv2d_139/bias:0: (<dtype: 'float32'>, (16,))\n",
            "idx=52 ==> tensorflow, conv2d_140/kernel:0: (<dtype: 'float32'>, (3, 3, 16, 16))\n",
            "idx=53 ==> tensorflow, conv2d_140/bias:0: (<dtype: 'float32'>, (16,))\n",
            "n1=54, n2=54\n",
            "tf x0 shape = (1, 128, 256, 16), some values = [[-0.4566052  -0.28543013 -0.21115358 -0.1222743  -0.46161556]\n",
            " [-0.4658901  -0.16889663 -0.28308398 -0.04657913 -0.4835524 ]\n",
            " [-0.05238245 -0.08155582 -0.15507492 -0.40920275 -0.23588187]\n",
            " [-0.03442941 -0.08689004 -0.2132103  -0.46286422  0.13467409]\n",
            " [-0.139199   -0.07587541 -0.32083476 -0.12369461  0.20571505]]\n",
            "(1, 8, 16, 32) 32\n",
            "(1, 16, 32, 24) 24\n",
            "(1, 32, 64, 24) 24\n",
            "(1, 64, 128, 16) 16\n",
            "idx 0, dtype = float32, float32, equal or not = True, True\n",
            "idx 1, dtype = float32, float32, equal or not = True, True\n",
            "idx 2, dtype = float32, float32, equal or not = True, True\n",
            "idx 3, dtype = float32, float32, equal or not = True, True\n",
            "idx 4, dtype = float32, float32, equal or not = True, True\n",
            "idx 5, dtype = float32, float32, equal or not = True, True\n",
            "idx 6, dtype = float32, float32, equal or not = True, True\n",
            "idx 7, dtype = float32, float32, equal or not = True, True\n",
            "idx 8, dtype = float32, float32, equal or not = True, True\n",
            "idx 9, dtype = float32, float32, equal or not = True, True\n",
            "idx 10, dtype = float32, float32, equal or not = True, True\n",
            "idx 11, dtype = float32, float32, equal or not = True, True\n",
            "idx 12, dtype = float32, float32, equal or not = True, True\n",
            "idx 13, dtype = float32, float32, equal or not = True, True\n",
            "idx 14, dtype = float32, float32, equal or not = True, True\n",
            "idx 15, dtype = float32, float32, equal or not = True, True\n",
            "idx 16, dtype = float32, float32, equal or not = True, True\n",
            "idx 17, dtype = float32, float32, equal or not = True, True\n",
            "idx 18, dtype = float32, float32, equal or not = True, True\n",
            "idx 19, dtype = float32, float32, equal or not = True, True\n",
            "idx 20, dtype = float32, float32, equal or not = True, True\n",
            "idx 21, dtype = float32, float32, equal or not = True, True\n",
            "idx 22, dtype = float32, float32, equal or not = True, True\n",
            "idx 23, dtype = float32, float32, equal or not = True, True\n",
            "idx 24, dtype = float32, float32, equal or not = True, True\n",
            "idx 25, dtype = float32, float32, equal or not = True, True\n",
            "idx 26, dtype = float32, float32, equal or not = True, True\n",
            "idx 27, dtype = float32, float32, equal or not = True, True\n",
            "idx 28, dtype = float32, float32, equal or not = True, True\n",
            "idx 29, dtype = float32, float32, equal or not = True, True\n",
            "idx 30, dtype = float32, float32, equal or not = True, True\n",
            "idx 31, dtype = float32, float32, equal or not = True, True\n",
            "idx 32, dtype = float32, float32, equal or not = True, True\n",
            "idx 33, dtype = float32, float32, equal or not = True, True\n",
            "idx 34, dtype = float32, float32, equal or not = True, True\n",
            "idx 35, dtype = float32, float32, equal or not = True, True\n",
            "idx 36, dtype = float32, float32, equal or not = True, True\n",
            "idx 37, dtype = float32, float32, equal or not = True, True\n",
            "idx 38, dtype = float32, float32, equal or not = True, True\n",
            "idx 39, dtype = float32, float32, equal or not = True, True\n",
            "idx 40, dtype = float32, float32, equal or not = True, True\n",
            "idx 41, dtype = float32, float32, equal or not = True, True\n",
            "idx 42, dtype = float32, float32, equal or not = True, True\n",
            "idx 43, dtype = float32, float32, equal or not = True, True\n",
            "idx 44, dtype = float32, float32, equal or not = True, True\n",
            "idx 45, dtype = float32, float32, equal or not = True, True\n",
            "idx 46, dtype = float32, float32, equal or not = True, True\n",
            "idx 47, dtype = float32, float32, equal or not = True, True\n",
            "idx 48, dtype = float32, float32, equal or not = True, True\n",
            "idx 49, dtype = float32, float32, equal or not = True, True\n",
            "idx 50, dtype = float32, float32, equal or not = True, True\n",
            "idx 51, dtype = float32, float32, equal or not = True, True\n",
            "idx 52, dtype = float32, float32, equal or not = True, True\n",
            "idx 53, dtype = float32, float32, equal or not = True, True\n",
            "\n",
            "**********************\n",
            "\n",
            "**********************\n",
            "\n",
            "Setting atol=1e-06, the results from TensorFlow and PyTorch are equal or not = True\n",
            "\n",
            "**********************\n",
            "\n",
            "**********************\n",
            "\n",
            "Setting atol=1e-07, the results from TensorFlow and PyTorch are equal or not = False\n",
            "\n",
            "------\n",
            "for feature x0 at shape (1, 128, 256, 16)\n",
            "is_equal = False\n",
            " feature x0: channl 0 has abs = 2.151479883139018e-08\n",
            "PS: black means diff <= thred 1e-07, red means diff > thred 1e-07\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x128 at 0x7F5C4024B6A0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAACACAIAAABr1yBdAAABi0lEQVR4nO3cOZLCMBAFUN3/0kw6ZeNCYFnbfy8iocrqTY0DSqGF1+gHAADgW7+tcBY/4DaDpLPkgCefnd2oZtpTVQDAWLYRAGB754XHCrQtqYV0pgAPslFALb2xEMkCAAAAAICTJ16feyXfhjh+9HOIxBb4nskBzM6cAgAS2HkAAAAA/vO2BFale1mAMmVVd2r36ruT9MMkjwEALqXOhBtW1fY/fT9+17B4IyooUYcF4DHuEwA4cT3SzDzFNM+TcCQ3TbQKo3QAAFOxnAAAQDq/CoA6pgX0puugq4daTifDMNoPIIBhDwAAAAAA3Xk9D6vSvbA1LQ4AAKFeF5+5Q1RZnsLNVZ97VZJgtizP9jzAwzQ9DSSXUfLZAQCgiqW5j+Q4J58duGY2MC/VCSzMCGMNKhWoZV4AAEAnWy3fWx0mm1SyHlVLLtUPMJpJnEvuexJtgFKKcQjAGy4HBlOCMJgmBMjmHgAAmJAljSM1QRMKKVqr9CsjAACasVzCka6ATmKbLfbgAFBKKeUP9Xpjncz/GWwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------\n",
            "for feature x1 at shape (1, 64, 128, 16)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature x2 at shape (1, 32, 64, 24)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature x3 at shape (1, 16, 32, 24)\n",
            "is_equal = True\n",
            "\n",
            "------\n",
            "for feature o0 at shape (1, 8, 16, 32)\n",
            "is_equal = True\n",
            "\n",
            "------\n",
            "for feature o1 at shape (1, 16, 32, 24)\n",
            "is_equal = True\n",
            "\n",
            "------\n",
            "for feature o2 at shape (1, 32, 64, 24)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature o3 at shape (1, 64, 128, 16)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature o4 at shape (1, 128, 256, 16)\n",
            "is_equal = False\n",
            "\n",
            "**********************\n",
            "\n",
            "**********************\n",
            "\n",
            "Setting atol=1e-08, the results from TensorFlow and PyTorch are equal or not = False\n",
            "\n",
            "------\n",
            "for feature x0 at shape (1, 128, 256, 16)\n",
            "is_equal = False\n",
            " feature x0: channl 0 has abs = 2.151479883139018e-08\n",
            "PS: black means diff <= thred 1e-08, red means diff > thred 1e-08\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x128 at 0x7F5C40249060>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------\n",
            "for feature x1 at shape (1, 64, 128, 16)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature x2 at shape (1, 32, 64, 24)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature x3 at shape (1, 16, 32, 24)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature o0 at shape (1, 8, 16, 32)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature o1 at shape (1, 16, 32, 24)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature o2 at shape (1, 32, 64, 24)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature o3 at shape (1, 64, 128, 16)\n",
            "is_equal = False\n",
            "\n",
            "------\n",
            "for feature o4 at shape (1, 128, 256, 16)\n",
            "is_equal = False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the results are always the same when using `float64` no matter what the tolerance `atol` is used in the fucntion `np.allclose`. But for `float32`, small tolerance `atol` will result in `Not Equal`.\n",
        "- `Setting atol=1e-06, the results from TensorFlow and PyTorch are equal or not = True`\n",
        "- `Setting atol=1e-07, the results from TensorFlow and PyTorch are equal or not = False`\n",
        "- `Setting atol=1e-08, the results from TensorFlow and PyTorch are equal or not = False`"
      ],
      "metadata": {
        "id": "4Lx7A2sJeoOG"
      }
    }
  ]
}
